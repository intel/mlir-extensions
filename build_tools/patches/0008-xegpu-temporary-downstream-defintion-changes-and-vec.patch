From 6a3b9a4936f774957b6a1cedcae40a355fb9670e Mon Sep 17 00:00:00 2001
From: Garra1980 <igor.zamyatin@intel.com>
Date: Wed, 20 Aug 2025 01:20:08 +0200
Subject: [PATCH] xegpu temporary downstream defintion changes and vec

---
 mlir/include/mlir/Dialect/XeGPU/IR/XeGPUOps.td      |  5 +++++
 mlir/lib/Conversion/VectorToXeGPU/VectorToXeGPU.cpp |  7 ++++++-
 mlir/lib/Dialect/XeGPU/IR/XeGPUOps.cpp              | 10 ++++++----
 3 files changed, 17 insertions(+), 5 deletions(-)

diff --git a/mlir/include/mlir/Dialect/XeGPU/IR/XeGPUOps.td b/mlir/include/mlir/Dialect/XeGPU/IR/XeGPUOps.td
index eb54d6887681..b849c6b97d9d 100644
--- a/mlir/include/mlir/Dialect/XeGPU/IR/XeGPUOps.td
+++ b/mlir/include/mlir/Dialect/XeGPU/IR/XeGPUOps.td
@@ -329,6 +329,7 @@ def XeGPU_LoadNdOp : XeGPU_Op<"load_nd", [
                        OptionalAttr<DenseI64ArrayAttr>: $const_offsets,
                        OptionalAttr<UnitAttr>: $packed,
                        OptionalAttr<DenseI64ArrayAttr>: $transpose,
+                       OptionalAttr<I32Attr>: $transpose_bit_width,
                        OptionalAttr<XeGPU_CacheHintAttr>: $l1_hint,
                        OptionalAttr<XeGPU_CacheHintAttr>: $l2_hint,
                        OptionalAttr<XeGPU_CacheHintAttr>: $l3_hint);
@@ -1260,5 +1261,9 @@ def XeGPU_MemDescSubviewOp: XeGPU_Op<"mem_desc_subview",
   let hasVerifier = 1;
 }

+def XeGPU_CompileHintOp : XeGPU_Op<"compile_hint", []> {
+  let summary = "prevents the compiler from scheduling.";
+  let assemblyFormat = [{ attr-dict }];
+}

 #endif // MLIR_DIALECT_XEGPU_IR_XEGPUOPS_TD
diff --git a/mlir/lib/Conversion/VectorToXeGPU/VectorToXeGPU.cpp b/mlir/lib/Conversion/VectorToXeGPU/VectorToXeGPU.cpp
index 819c2e5973ff..545f1d77156c 100644
--- a/mlir/lib/Conversion/VectorToXeGPU/VectorToXeGPU.cpp
+++ b/mlir/lib/Conversion/VectorToXeGPU/VectorToXeGPU.cpp
@@ -485,7 +485,9 @@ struct TransferReadLowering : public OpRewritePattern<vector::TransferReadOp> {
     // By default, no specific caching policy is assigned.
     xegpu::CachePolicyAttr hint = nullptr;
     auto loadOp = xegpu::LoadNdOp::create(rewriter, loc, vecTy, ndDesc,
+                                          ValueRange(), DenseI64ArrayAttr(),
                                           /*packed=*/nullptr, transposeAttr,
+                                          /*transpose_bit_width*/nullptr,
                                           /*l1_hint=*/hint,
                                           /*l2_hint=*/hint, /*l3_hint=*/hint);
     rewriter.replaceOp(readOp, loadOp);
@@ -569,7 +571,10 @@ struct LoadLowering : public OpRewritePattern<vector::LoadOp> {
     // By default, no specific caching policy is assigned.
     xegpu::CachePolicyAttr hint = nullptr;
     auto loadNdOp = xegpu::LoadNdOp::create(
-        rewriter, loc, vecTy, ndDesc, /*packed=*/nullptr, /*transpose=*/nullptr,
+        rewriter, loc, vecTy, ndDesc,
+        ValueRange(), DenseI64ArrayAttr(),
+        /*packed=*/nullptr, /*transpose=*/nullptr,
+        /*transpose_bit_width*/nullptr,
         /*l1_hint=*/hint,
         /*l2_hint=*/hint, /*l3_hint=*/hint);
     rewriter.replaceOp(loadOp, loadNdOp);
diff --git a/mlir/lib/Dialect/XeGPU/IR/XeGPUOps.cpp b/mlir/lib/Dialect/XeGPU/IR/XeGPUOps.cpp
index 906c71d8b8da..ecee53c56a54 100644
--- a/mlir/lib/Dialect/XeGPU/IR/XeGPUOps.cpp
+++ b/mlir/lib/Dialect/XeGPU/IR/XeGPUOps.cpp
@@ -78,6 +78,7 @@ static bool isWriteHintOrNone(const CachePolicyAttr &attr) {
     return true;
   auto kind = attr.getValue();
   return kind == CachePolicy::CACHED || kind == CachePolicy::UNCACHED ||
+         kind == CachePolicy::STREAMING ||
          kind == CachePolicy::WRITE_BACK || kind == CachePolicy::WRITE_THROUGH;
 }

@@ -438,8 +439,8 @@ void LoadNdOp::build(OpBuilder &builder, OperationState &state, Type retType,
                      xegpu::CachePolicyAttr l3_hint) {

   return build(builder, state, retType, tensorDesc, ValueRange(),
-               DenseI64ArrayAttr(), packed, transpose, l1_hint, l2_hint,
-               l3_hint);
+               DenseI64ArrayAttr(), packed, transpose, nullptr, /*transpose_bit_width*/
+               l1_hint, l2_hint, l3_hint);
 }

 void LoadNdOp::build(OpBuilder &builder, OperationState &state, Type retType,
@@ -455,7 +456,8 @@ void LoadNdOp::build(OpBuilder &builder, OperationState &state, Type retType,
   auto staticOffsetsAttr = builder.getDenseI64ArrayAttr(staticOffsets);

   build(builder, state, retType, tensorDesc, dynamicOffsets, staticOffsetsAttr,
-        packed, transpose, l1_hint, l2_hint, l3_hint);
+        packed, transpose, nullptr, /*transpose_bit_width*/
+        l1_hint, l2_hint, l3_hint);
 }

 LogicalResult LoadNdOp::verify() {
@@ -517,7 +519,7 @@ LogicalResult LoadNdOp::verify() {
       mlir::emitWarning(getLoc()) << "Invalid transpose attr. It is ignored.";
   }

-  if (getPacked()) {
+  if (getPacked() || getTransposeBitWidth() == 32) {
     if (tdescTy.getRank() == 2) {
       const int axis = 0;
       auto vnni_factor = valueShape.back();
--
2.34.1
