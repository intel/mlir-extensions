non_pvc_excludes = [
                    'gemm_1024x1024xf16.mlir',
                    'gemm_1024x1024xbf16.mlir',
                    'gemm_1024x1024xf16.using.updateoffset.mlir',
                    'gemm_1024x1016x1016_f16_f16_f32.mlir',
                    'load2d-padding-f32.mlir',
                    'load2d-padding.mlir',
                    'gemm_4kx4kx4k_f16_f16_f16.mlir',
                    'gemm_4kx4kx4k_f16_f16_f16_w_8x32xf16_stores.mlir',
                    'gemm_with_transposed_B_1kx1kx1k_f16_f16_f32.mlir',
                    'flash_attention_fwd.mlir',
                    'gemm_4kx4kx4k_f16_f16_f16_w_simple_B_prefetch.mlir',
                 ]

local_excludes = [
                    'gemm_4kx4kx4k_dpas_sized_loads_f16_f16_f32.mlir',
                    'unranked_memref.vc.mlir',  # host code lowering has issues. spirv binary generated is identical to ranked_dynamic_memref.vc.mlir
                    'xegpu-to-vc.mlir', # 128xf32 is not a supported 1D vector length for load/store
                    'optimize_transpose_array_length.mlir', # temp exclude until 1:N conversion is fixed
                 ]

if(not config.imex_enable_pvc_target and not config.imex_enable_fs_simulator):
  local_excludes += non_pvc_excludes

slow_simulator_tests = [
    'gemm_4kx4kx4k_bf16_bf16_f32_xetla_like_load_store_prefetch.mlir',
    'gemm_4kx4kx4k_f16_f16_f16_w_8x32xf16_stores.mlir',
    'gemm_4kx4kx4k_f16_f16_f16.mlir',
    'gemm_4kx4kx4k_f16_f16_f16_w_simple_B_prefetch.mlir',
    'gemm_1024x1016x1016_f16_f16_f32.mlir',
    'gemm_1024x1024xf16.using.updateoffset.mlir',
    'gemm_with_transposed_B_1kx1kx1k_f16_f16_f32.mlir',
    'gemm_1024x1024xf16.mlir',
    'gemm_1024x1024xbf16.mlir',
    'flash_attention_fwd.mlir',
]

if(config.imex_enable_fs_simulator):
  local_excludes += slow_simulator_tests

if(not config.imex_enable_excluded_tests):
  config.excludes.update(local_excludes)
